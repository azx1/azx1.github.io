---
layout: post
header-img: "img/post-bg.jpg"
title: "深度学习在图像超分辨率重建中的应用"
author: "taigw"
author-link: "https://www.zhihu.com/people/travelsea/activities"
date: "2017-12-30"
reprint-link: "https://zhuanlan.zhihu.com/p/25532538?utm_source=tuicool&utm_medium=referral"
tags:
    - 转载
    - 深度学习
---
*目录
{:toc}
<div class="RichText PostIndex-content av-paddingSide av-card"><p>超分辨率技术（Super-Resolution）是指从观测到的低分辨率图像重建出相应的高分辨率图像，在监控设备、卫星图像和医学影像等领域都有重要的应用价值。SR可分为两类:从多张低分辨率图像重建出高分辨率图像和从单张低分辨率图像重建出高分辨率图像。基于深度学习的SR，主要是基于单张低分辨率的重建方法，即Single Image Super-Resolution (SISR)。</p><p>SISR是一个逆问题，对于一个低分辨率图像，可能存在许多不同的高分辨率图像与之对应，因此通常在求解高分辨率图像时会加一个先验信息进行规范化约束。在传统的方法中，这个先验信息可以通过若干成对出现的低-高分辨率图像的实例中学到。而基于深度学习的SR通过神经网络直接学习分辨率图像到高分辨率图像的端到端的映射函数。</p><p>本文介绍几个较新的基于深度学习的SR方法，包括SRCNN，DRCN, ESPCN，VESPCN和SRGAN等。  </p><h2>1，SRCNN<br></h2><p> Super-Resolution Convolutional Neural Network （SRCNN, PAMI 2016, <a href="http://link.zhihu.com/?target=http%3A//mmlab.ie.cuhk.edu.hk/projects/SRCNN.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">代码</a>)是较早地提出的做SR的卷积神经网络。该网络结构十分简单，仅仅用了三个卷积层。</p><p></p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-82423d397a228f3b4522769ae9a85e83_b.jpg"|prepend:baseurl }}" data-rawwidth="1282" data-rawheight="454" class="origin_image zh-lightbox-thumb" width="1282" data-original="https://pic3.zhimg.com/v2-82423d397a228f3b4522769ae9a85e83_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-82423d397a228f3b4522769ae9a85e83_hd.jpg"|prepend:baseurl }}" data-rawwidth="1282" data-rawheight="454" class="origin_image zh-lightbox-thumb lazy" width="1282" data-original="https://pic3.zhimg.com/v2-82423d397a228f3b4522769ae9a85e83_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-82423d397a228f3b4522769ae9a85e83_b.jpg"></figure>该方法对于一个低分辨率图像，先使用双三次（bicubic）插值将其放大到目标大小，再通过三层卷积网络做非线性映射，得到的结果作为高分辨率图像输出。作者将三层卷积的结构解释成与传统SR方法对应的三个步骤：图像块的提取和特征表示，特征非线性映射和最终的重建。<p></p><p>三个卷积层使用的卷积核的大小分为为9x9, 1x1和5x5，前两个的输出特征个数分别为64和32. 该文章分别用Timofte数据集（包含91幅图像）和ImageNet大数据集进行训练。相比于双三次插值和传统的稀疏编码方法，SRCNN得到的高分辨率图像更加清晰，下图是一个放大倍数为3的例子。</p><p></p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-a22efaa701ce06df06e2b3bf520609d3_b.jpg"|prepend:baseurl }}" data-rawwidth="670" data-rawheight="938" class="origin_image zh-lightbox-thumb" width="670" data-original="https://pic1.zhimg.com/v2-a22efaa701ce06df06e2b3bf520609d3_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-a22efaa701ce06df06e2b3bf520609d3_hd.jpg"|prepend:baseurl }}" data-rawwidth="670" data-rawheight="938" class="origin_image zh-lightbox-thumb lazy" width="670" data-original="https://pic1.zhimg.com/v2-a22efaa701ce06df06e2b3bf520609d3_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-a22efaa701ce06df06e2b3bf520609d3_b.jpg"></figure>对SR的质量进行定量评价常用的两个指标是PSNR(Peak Signal-to-Noise Ratio)和SSIM（Structure Similarity Index）。这两个值越高代表重建结果的像素值和金标准越接近，下图表明，在不同的放大倍数下，SRCNN都取得比传统方法好的效果。<figure><noscript>&lt;img src="{{ "/img/post-img/v2-b4ebbda16d8671d39abe06e6cad2d65e_b.jpg"|prepend:baseurl }}" data-rawwidth="1264" data-rawheight="240" class="origin_image zh-lightbox-thumb" width="1264" data-original="https://pic2.zhimg.com/v2-b4ebbda16d8671d39abe06e6cad2d65e_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-b4ebbda16d8671d39abe06e6cad2d65e_hd.jpg"|prepend:baseurl }}" data-rawwidth="1264" data-rawheight="240" class="origin_image zh-lightbox-thumb lazy" width="1264" data-original="https://pic2.zhimg.com/v2-b4ebbda16d8671d39abe06e6cad2d65e_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-b4ebbda16d8671d39abe06e6cad2d65e_b.jpg"></figure>2， DRCN<p></p><p>SRCNN的层数较少，同时感受野也较小（13x13）。DRCN (Deeply-Recursive Convolutional Network for Image Super-Resolution, CVPR 2016, <a href="http://link.zhihu.com/?target=http%3A//cv.snu.ac.kr/research/DRCN/" class=" wrap external" target="_blank" rel="nofollow noreferrer">代码</a>)提出使用更多的卷积层增加网络感受野（41x41），同时为了避免过多网络参数，该文章提出使用递归神经网络（RNN）。网络的基本结构如下：</p><p></p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-cb1c3003163537f45d5a2ab532d7c4db_b.jpg"|prepend:baseurl }}" data-rawwidth="1318" data-rawheight="436" class="origin_image zh-lightbox-thumb" width="1318" data-original="https://pic1.zhimg.com/v2-cb1c3003163537f45d5a2ab532d7c4db_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-cb1c3003163537f45d5a2ab532d7c4db_hd.jpg"|prepend:baseurl }}" data-rawwidth="1318" data-rawheight="436" class="origin_image zh-lightbox-thumb lazy" width="1318" data-original="https://pic1.zhimg.com/v2-cb1c3003163537f45d5a2ab532d7c4db_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-cb1c3003163537f45d5a2ab532d7c4db_b.jpg"></figure>与SRCNN类似，该网络分为三个模块，第一个是Embedding network，相当于特征提取，第二个是Inference network, 相当于特征的非线性变换，第三个是Reconstruction network,即从特征图像得到最后的重建结果。其中的Inference network是一个递归网络，即数据循环地通过该层多次。将这个循环进行展开，就等效于使用同一组参数的多个串联的卷积层，如下图所示：<p></p><p></p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-51635e93b7f0eb849cf3d5e19db80d79_b.jpg"|prepend:baseurl }}" data-rawwidth="1344" data-rawheight="298" class="origin_image zh-lightbox-thumb" width="1344" data-original="https://pic2.zhimg.com/v2-51635e93b7f0eb849cf3d5e19db80d79_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-51635e93b7f0eb849cf3d5e19db80d79_hd.jpg"|prepend:baseurl }}" data-rawwidth="1344" data-rawheight="298" class="origin_image zh-lightbox-thumb lazy" width="1344" data-original="https://pic2.zhimg.com/v2-51635e93b7f0eb849cf3d5e19db80d79_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-51635e93b7f0eb849cf3d5e19db80d79_b.jpg"></figure>其中的<img src="https://www.zhihu.com/equation?tex=H_1"  alt="H_1" eeimg="1">到<img src="https://www.zhihu.com/equation?tex=H_D" alt="H_D" eeimg="1">是D个共享参数的卷积层。DRCN将每一层的卷积结果都通过同一个Reconstruction Net得到一个重建结果，从而共得到D个重建结果，再把它们加权平均得到最终的输出。另外，受到ResNet的启发，DRCN通过skip connection将输入图像与<img src="https://www.zhihu.com/equation?tex=H_d" alt="H_d" eeimg="1">的输出相加后再作为Reconstruction Net的输入，相当于使Inference Net去学习高分辨率图像与低分辨率图像的差，即恢复图像的高频部分。<p></p><p>实验部分，DRCN也使用了包含91张图像的Timofte数据集进行训练。得到的效果比SRCNN有了较大提高。</p><h2><figure><noscript>&lt;img src="{{ "/img/post-img/v2-c40759b372c4aa962a762d1db470da06_b.jpg"|prepend:baseurl }}" data-rawwidth="1328" data-rawheight="508" class="origin_image zh-lightbox-thumb" width="1328" data-original="https://pic3.zhimg.com/v2-c40759b372c4aa962a762d1db470da06_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-c40759b372c4aa962a762d1db470da06_hd.jpg"|prepend:baseurl }}" data-rawwidth="1328" data-rawheight="508" class="origin_image zh-lightbox-thumb lazy" width="1328" data-original="https://pic3.zhimg.com/v2-c40759b372c4aa962a762d1db470da06_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-c40759b372c4aa962a762d1db470da06_b.jpg"></figure>3， ESPCN</h2><p>在SRCNN和DRCN中，低分辨率图像都是先通过上采样插值得到与高分辨率图像同样的大小，再作为网络输入，意味着卷积操作在较高的分辨率上进行，相比于在低分辨率的图像上计算卷积，会降低效率。 ESPCN(Real-Time
Single Image and Video Super-Resolution Using an Efficient Sub-Pixel
Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation, arxiv 2016）提出使用视频中的时间序列图像进行高分辨率重建，并且能达到实时处理的效率要求。其方法示意图如下，主要包括三个方面： <br></p><p></p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-ba1f3f799d577cf458dee287a8ab8951_b.jpg"|prepend:baseurl }}" data-rawwidth="480" data-rawheight="181" class="origin_image zh-lightbox-thumb" width="480" data-original="https://pic4.zhimg.com/v2-ba1f3f799d577cf458dee287a8ab8951_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-ba1f3f799d577cf458dee287a8ab8951_hd.jpg"|prepend:baseurl }}" data-rawwidth="480" data-rawheight="181" class="origin_image zh-lightbox-thumb lazy" width="480" data-original="https://pic4.zhimg.com/v2-ba1f3f799d577cf458dee287a8ab8951_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-ba1f3f799d577cf458dee287a8ab8951_b.jpg"></figure> 一是纠正相邻帧的位移偏差，即先通过Motion estimation估计出位移，然后利用位移参数对相邻帧进行空间变换，将二者对齐。二是把对齐后的相邻若干帧叠放在一起，当做一个三维数据，在低分辨率的三维数据上使用三维卷积，得到的结果大小为<img src="https://www.zhihu.com/equation?tex=r%5E2%5Ctimes+H%5Ctimes+W" alt="r^2\times H\times W" eeimg="1">。三是利用ESPCN的思想将该卷积结果重新排列得到大小为<img src="https://www.zhihu.com/equation?tex=1%5Ctimes+rH%5Ctimes+rW" alt="1\times rH\times rW" eeimg="1">的高分辨率图像。<p></p><p>Motion estimation这个过程可以通过传统的光流算法来计算，DeepMind 提出了一个Spatial Transformer Networks, 通过CNN来估计空间变换参数。VESPCN使用了这个方法，并且使用多尺度的Motion estimation：先在比输入图像低的分辨率上得到一个初始变换，再在与输入图像相同的分辨率上得到更精确的结果，如下图所示：</p><p></p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-cc6843a2f89818b0b4630917563cedae_b.jpg"|prepend:baseurl }}" data-rawwidth="366" data-rawheight="293" class="content_image" width="366"&gt;</noscript><img src="{{ "/img/post-img/v2-cc6843a2f89818b0b4630917563cedae_hd.jpg"|prepend:baseurl }}" data-rawwidth="366" data-rawheight="293" class="content_image lazy" width="366" data-actualsrc="https://pic1.zhimg.com/v2-cc6843a2f89818b0b4630917563cedae_b.jpg"></figure> 由于SR重建和相邻帧之间的位移估计都通过神经网路来实现，它们可以融合在一起进行端到端的联合训练。为此，VESPCN使用的损失函数如下：<p></p><p></p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-81ffc0a97771d2ac7fee62a806b7d906_b.jpg"|prepend:baseurl }}" data-rawwidth="450" data-rawheight="112" class="origin_image zh-lightbox-thumb" width="450" data-original="https://pic1.zhimg.com/v2-81ffc0a97771d2ac7fee62a806b7d906_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-81ffc0a97771d2ac7fee62a806b7d906_hd.jpg"|prepend:baseurl }}" data-rawwidth="450" data-rawheight="112" class="origin_image zh-lightbox-thumb lazy" width="450" data-original="https://pic1.zhimg.com/v2-81ffc0a97771d2ac7fee62a806b7d906_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-81ffc0a97771d2ac7fee62a806b7d906_b.jpg"></figure> 第一项是衡量重建结果和金标准之间的差异，第二项是衡量相邻输入帧在空间对齐后的差异，第三项是平滑化空间位移场。下图展示了使用Motion Compensation 后，相邻帧之间对得很整齐，它们的差值图像几乎为0.<br><p></p><p></p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-50981a508d755aee2f69b026082c2cec_b.jpg"|prepend:baseurl }}" data-rawwidth="503" data-rawheight="424" class="origin_image zh-lightbox-thumb" width="503" data-original="https://pic3.zhimg.com/v2-50981a508d755aee2f69b026082c2cec_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-50981a508d755aee2f69b026082c2cec_hd.jpg"|prepend:baseurl }}" data-rawwidth="503" data-rawheight="424" class="origin_image zh-lightbox-thumb lazy" width="503" data-original="https://pic3.zhimg.com/v2-50981a508d755aee2f69b026082c2cec_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-50981a508d755aee2f69b026082c2cec_b.jpg"></figure>从下图可以看出，使用了Motion Compensation，重建出的高分辨率视频图像更加清晰。<figure><noscript>&lt;img src="{{ "/img/post-img/v2-53f68cf3f91dfdd11c91cbad6c4a137f_b.jpg"|prepend:baseurl }}" data-rawwidth="480" data-rawheight="198" class="origin_image zh-lightbox-thumb" width="480" data-original="https://pic1.zhimg.com/v2-53f68cf3f91dfdd11c91cbad6c4a137f_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-53f68cf3f91dfdd11c91cbad6c4a137f_hd.jpg"|prepend:baseurl }}" data-rawwidth="480" data-rawheight="198" class="origin_image zh-lightbox-thumb lazy" width="480" data-original="https://pic1.zhimg.com/v2-53f68cf3f91dfdd11c91cbad6c4a137f_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-53f68cf3f91dfdd11c91cbad6c4a137f_b.jpg"></figure>5， SRGAN<p></p><p>SRGAN (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, <a href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1609.04802" class=" wrap external" target="_blank" rel="nofollow noreferrer">arxiv</a>, 21 Nov, 2016)将生成式对抗网络（GAN)用于SR问题。其出发点是传统的方法一般处理的是较小的放大倍数，当图像的放大倍数在4以上时，很容易使得到的结果显得过于平滑，而缺少一些细节上的真实感。因此SRGAN使用GAN来生成图像中的细节。</p><p>传统的方法使用的代价函数一般是最小均方差（MSE），即</p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-6bbe689fffae1e3991a392caaeb8a1bd_b.jpg"|prepend:baseurl }}" data-rawwidth="367" data-rawheight="69" class="content_image" width="367"&gt;</noscript><img src="{{ "/img/post-img/v2-6bbe689fffae1e3991a392caaeb8a1bd_hd.jpg"|prepend:baseurl }}" data-rawwidth="367" data-rawheight="69" class="content_image lazy" width="367" data-actualsrc="https://pic1.zhimg.com/v2-6bbe689fffae1e3991a392caaeb8a1bd_b.jpg"></figure><p> 该代价函数使重建结果有较高的信噪比，但是缺少了高频信息，出现过度平滑的纹理。SRGAN认为，应当使重建的高分辨率图像与真实的高分辨率图像无论是低层次的像素值上，还是高层次的抽象特征上，和整体概念和风格上，都应当接近。整体概念和风格如何来评估呢？可以使用一个判别器，判断一副高分辨率图像是由算法生成的还是真实的。如果一个判别器无法区分出来，那么由算法生成的图像就达到了以假乱真的效果。</p><p>因此，该文章将代价函数改进为</p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-8fc1773957c067b51082080af51f1dd6_b.jpg"|prepend:baseurl }}" data-rawwidth="284" data-rawheight="58" class="content_image" width="284"&gt;</noscript><img src="{{ "/img/post-img/v2-8fc1773957c067b51082080af51f1dd6_hd.jpg"|prepend:baseurl }}" data-rawwidth="284" data-rawheight="58" class="content_image lazy" width="284" data-actualsrc="https://pic2.zhimg.com/v2-8fc1773957c067b51082080af51f1dd6_b.jpg"></figure> 第一部分是基于内容的代价函数，第二部分是基于对抗学习的代价函数。基于内容的代价函数除了上述像素空间的最小均方差以外，又包含了一个基于特征空间的最小均方差，该特征是利用VGG网络提取的图像高层次特征：<figure><noscript>&lt;img src="{{ "/img/post-img/v2-aea70f5cc362f193fd8022d18e64de72_b.jpg"|prepend:baseurl }}" data-rawwidth="349" data-rawheight="103" class="content_image" width="349"&gt;</noscript><img src="{{ "/img/post-img/v2-aea70f5cc362f193fd8022d18e64de72_hd.jpg"|prepend:baseurl }}" data-rawwidth="349" data-rawheight="103" class="content_image lazy" width="349" data-actualsrc="https://pic2.zhimg.com/v2-aea70f5cc362f193fd8022d18e64de72_b.jpg"></figure>对抗学习的代价函数是基于判别器输出的概率：<figure><noscript>&lt;img src="{{ "/img/post-img/v2-5ca7ae9a7648006aa37c502e0e52a2ae_b.jpg"|prepend:baseurl }}" data-rawwidth="268" data-rawheight="70" class="content_image" width="268"&gt;</noscript><img src="{{ "/img/post-img/v2-5ca7ae9a7648006aa37c502e0e52a2ae_hd.jpg"|prepend:baseurl }}" data-rawwidth="268" data-rawheight="70" class="content_image lazy" width="268" data-actualsrc="https://pic2.zhimg.com/v2-5ca7ae9a7648006aa37c502e0e52a2ae_b.jpg"></figure> 其中<img src="https://www.zhihu.com/equation?tex=D_%7B%5Ctheta+D%7D%28%29" alt="D_{\theta D}()" eeimg="1">是一个图像属于真实的高分辨率图像的概率。<img src="https://www.zhihu.com/equation?tex=G_%7B%5Ctheta+G%7D%28I%5E%7BLR%7D%29" alt="G_{\theta G}(I^{LR})" eeimg="1">是重建的高分辨率图像。SRGAN使用的生成式网络和判别式网络分别如下：<p></p><p></p><figure><noscript>&lt;img src="{{ "/img/post-img/v2-e200a42d28f1271d0d3f9f0648f0bd90_b.jpg"|prepend:baseurl }}" data-rawwidth="1025" data-rawheight="543" class="origin_image zh-lightbox-thumb" width="1025" data-original="https://pic2.zhimg.com/v2-e200a42d28f1271d0d3f9f0648f0bd90_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-e200a42d28f1271d0d3f9f0648f0bd90_hd.jpg"|prepend:baseurl }}" data-rawwidth="1025" data-rawheight="543" class="origin_image zh-lightbox-thumb lazy" width="1025" data-original="https://pic2.zhimg.com/v2-e200a42d28f1271d0d3f9f0648f0bd90_r.jpg" data-actualsrc="https://pic2.zhimg.com/v2-e200a42d28f1271d0d3f9f0648f0bd90_b.jpg"></figure> 该方法的实验结果如下<figure><noscript>&lt;img src="{{ "/img/post-img/v2-d6d1847768f34c335bf1d2821e208ddd_b.jpg"|prepend:baseurl }}" data-rawwidth="880" data-rawheight="359" class="origin_image zh-lightbox-thumb" width="880" data-original="https://pic3.zhimg.com/v2-d6d1847768f34c335bf1d2821e208ddd_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-d6d1847768f34c335bf1d2821e208ddd_hd.jpg"|prepend:baseurl }}" data-rawwidth="880" data-rawheight="359" class="origin_image zh-lightbox-thumb lazy" width="880" data-original="https://pic3.zhimg.com/v2-d6d1847768f34c335bf1d2821e208ddd_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-d6d1847768f34c335bf1d2821e208ddd_b.jpg"></figure>从定量评价结果上来看，PSNR和SSIM这两个指标评价的是重建结果和金标准在像素值空间的差异。SRGAN得到的评价值不是最高。但是对于MOS（mean opinion score）的评价显示，SRGAN生成的高分辨率图像看起来更真实。 <figure><noscript>&lt;img src="{{ "/img/post-img/v2-44d3fa43704296c4642f4704f4658d24_b.jpg"|prepend:baseurl }}" data-rawwidth="853" data-rawheight="111" class="origin_image zh-lightbox-thumb" width="853" data-original="https://pic4.zhimg.com/v2-44d3fa43704296c4642f4704f4658d24_r.jpg"&gt;</noscript><img src="{{ "/img/post-img/v2-44d3fa43704296c4642f4704f4658d24_hd.jpg"|prepend:baseurl }}" data-rawwidth="853" data-rawheight="111" class="origin_image zh-lightbox-thumb lazy" width="853" data-original="https://pic4.zhimg.com/v2-44d3fa43704296c4642f4704f4658d24_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-44d3fa43704296c4642f4704f4658d24_b.jpg"></figure>参考资料<p></p><p>1， Dong, Chao, et al. "Image super-resolution using deep convolutional networks." <i>IEEE transactions on pattern analysis and machine intelligence</i> 38.2 (2016): 295-307.</p><p>2,   Kim, Jiwon, Jung Kwon Lee, and Kyoung Mu Lee. "Deeply-recursive convolutional network for image super-resolution." <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>. 2016.</p><p>3,  Shi, Wenzhe, et al. 
"Real-time single image and video super-resolution using an efficient 
sub-pixel convolutional neural network." <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>. 2016.</p><p>4,  Caballero, Jose, et al. "Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation." <i>arXiv preprint arXiv:1611.05250</i> (2016).</p><p>5, Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. "Spatial transformer networks." <i>Advances in Neural Information Processing Systems</i>. 2015.</p><p>6, Ledig, Christian, et al. "Photo-realistic single image super-resolution using a generative adversarial network." <i>arXiv preprint arXiv:1609.04802</i> (2016).</p><p>7,  <a href="https://zhuanlan.zhihu.com/p/25201511" class="internal">深度对抗学习在图像分割和超分辨率中的应用 - 知乎专栏</a></p></div>
